{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from os.path import expanduser\n",
    "from sklearn import mixture\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "mpl.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concreteness correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = expanduser('~')\n",
    "datafile = home + '/Research/protocols/data/noun_raw_data_norm_122218.tsv'\n",
    "\n",
    "data = pd.read_csv(datafile, sep=\"\\t\")\n",
    "\n",
    "# Split the datasets into train, dev, test\n",
    "data = data[data['Split'].isin(['train', 'dev'])]\n",
    "\n",
    "path = home + \"/Research/protocols/data/concreteness.tsv\"\n",
    "concreteness = pd.read_csv(path, sep=\"\\t\")\n",
    "list_of_lemmas = concreteness['Word'].values.tolist()\n",
    "\n",
    "abs_conc = data.groupby('Lemma')['Is.Abstract.Norm'].mean().to_frame().reset_index()\n",
    "abs_conc['Concreteness'] = abs_conc['Lemma'].map(lambda x: concreteness[concreteness['Word'] == x.lower()]['Conc.M'].values[0] if x.lower() in list_of_lemmas else -1)\n",
    "ini = len(abs_conc)\n",
    "abs_conc = abs_conc[abs_conc['Concreteness'] != -1]\n",
    "print(len(abs_conc) / ini)\n",
    "print(\"Spearman correlation: \", np.round(spearmanr(abs_conc['Is.Abstract.Norm'].values, abs_conc['Concreteness'].values)[0], 2))\n",
    "print(\"Pearson correlation: \", np.round(pearsonr(abs_conc['Is.Abstract.Norm'].values, abs_conc['Concreteness'].values)[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = home + \"/Research/protocols/data/verbs-English.lcs\"\n",
    "# lcs = LexicalConceptualStructureLexicon(path)\n",
    "# with open(home + '/Downloads/lcs.pkl', 'wb') as f:\n",
    "#     pickle.dump(lcs, f)\n",
    "# print(\"Pickled\")\n",
    "with open(home + '/Downloads/lcs.pkl', 'rb') as f:\n",
    "    lcs = pickle.load(f)\n",
    "# Read annotations\n",
    "datafile = home + \"/Research/protocols/data/pred_raw_data_norm_122218.tsv\"\n",
    "\n",
    "data = pd.read_csv(datafile, sep=\"\\t\")\n",
    "\n",
    "# Split the datasets into train, dev, test\n",
    "data = data[data['Split'].isin(['train', 'dev'])]\n",
    "\n",
    "dyn_lcs = data.groupby('Lemma')['Is.Dynamic.Norm'].apply(list).to_frame().reset_index()\n",
    "dyn_lcs['lcs'] = dyn_lcs['Lemma'].map(lambda x: lcs.eventive(x.lower()) if x.lower() in lcs.verbs else -1)\n",
    "num_of_lemmas = len(dyn_lcs)\n",
    "dyn_lcs = dyn_lcs[dyn_lcs['lcs'] != -1]\n",
    "dyn_lcs.set_index('Lemma', inplace=True)\n",
    "dyn_lcs['dyn'] = dyn_lcs['Is.Dynamic.Norm'].apply(lambda x: [a > 0 for a in x])\n",
    "dyn_lcs['comp'] = dyn_lcs.apply(lambda x: 1 if set(x['dyn']).intersection(set(x['lcs'])) else 0, axis=1)\n",
    "\n",
    "print(\"Percentage of lemmas found in lcs database:\", len(dyn_lcs) / num_of_lemmas)\n",
    "print(\"They share at least one sense:\", sum(dyn_lcs['comp']) / len(dyn_lcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = home + \"/Research/protocols/data/pred_raw_data_norm_122218.tsv\"\n",
    "pred_data = pd.read_csv(datafile, sep=\"\\t\")\n",
    "\n",
    "datafile_ = home + \"/Research/protocols/data/it-happened_eng_ud1.2_07092017_normalized.tsv\"\n",
    "fact_data = pd.read_csv(datafile_, sep=\"\\t\")\n",
    "\n",
    "pred_data = pred_data[pred_data['Split'].isin(['train', 'dev'])]\n",
    "pred_data['Sentence.ID'] = pred_data['Sentence.ID'].str.replace('sent_', '', regex=False)\n",
    "pred_data['Unique.ID'] = pred_data.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x[\"Root.Token\"]), axis=1)\n",
    "pred_data = pred_data.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "\n",
    "fact_data = fact_data[fact_data['Split'].isin(['train', 'dev'])]\n",
    "fact_data['Unique.ID'] = fact_data.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x[\"Pred.Token\"] - 1), axis=1)\n",
    "fact_data = fact_data.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "\n",
    "hyp_fact = pred_data.loc[:, ['Unique.ID', 'Is.Hypothetical.Norm', 'Is.Particular.Norm', 'Is.Dynamic.Norm']]\n",
    "fact_ids = fact_data['Unique.ID'].tolist()\n",
    "hyp_fact['Happened.Norm'] = hyp_fact['Unique.ID'].apply(lambda x: fact_data[fact_data['Unique.ID'] == x]['Happened.Norm'].iloc[0] if x in fact_ids else None)\n",
    "hyp_fact2 = hyp_fact.dropna()\n",
    "print(np.round(len(hyp_fact2) / len(hyp_fact), 2))\n",
    "# asdf = hyp_fact2[(hyp_fact2['Is.Hypothetical.Norm']>1) & (hyp_fact2['Happened.Norm']<-1)]\n",
    "for attr in ['Is.Hypothetical.Norm', 'Is.Particular.Norm', 'Is.Dynamic.Norm']:\n",
    "    print(attr)\n",
    "    print(\"Spearman correlation: \", np.round(spearmanr(hyp_fact2[attr].values, hyp_fact2['Happened.Norm'].values)[0], 2))\n",
    "    print(\"Pearson correlation: \", np.round(pearsonr(hyp_fact2[attr].values, hyp_fact2['Happened.Norm'].values)[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [39.2, 51.6, 44.8, 58.2, 54.8, 58.0]\n",
    "y_kind = [26.0, 42.0, 33.1, 47.9, 45.6, 48.0]\n",
    "y_abs = [49.2, 34.4, 46.9, 55.8, 51.6, 56.2]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Argument correlation scores')\n",
    "ax = plt.subplot(111)\n",
    "# ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('Pearson correlation')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_kind, width=0.2, color='g', align='center', label='Kind')\n",
    "ax.bar(ind + 0.2, y_abs, width=0.2, color='r', align='center', label='Abstract')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.savefig('bars-arg-pear.png')\n",
    "\n",
    "\n",
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [16.3, 20.9, 20.3, 27.5, 24.7, 28.5]\n",
    "y_hyp = [13.8, 38.3, 22.9, 42.2, 38.8, 42.0]\n",
    "y_dyn = [33.2, 31.5, 29.4, 38.3, 37.5, 38.8]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Predicate correlation scores')\n",
    "ax = plt.subplot(111)\n",
    "# ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('Pearson correlation')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_hyp, width=0.2, color='g', align='center', label='Hypothetical')\n",
    "ax.bar(ind + 0.2, y_dyn, width=0.2, color='r', align='center', label='Dynamic')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.savefig('bars-pred-pear.png')\n",
    "\n",
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [7.9, 13.1, 9.7, 15.7, 13.5, 15.8]\n",
    "y_kind = [2.1, 8.9, 2.6, 11.6, 10.8, 11.4]\n",
    "y_abs = [14.1, 6.3, 11.9, 17.3, 15.2, 17.7]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Argument R1 scores')\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('R1')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_kind, width=0.2, color='g', align='center', label='Kind')\n",
    "ax.bar(ind + 0.2, y_abs, width=0.2, color='r', align='center', label='Abstract')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.savefig('bars-arg-r1.png')\n",
    "\n",
    "\n",
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [1.2, 2.7, 2.2, 3.9, 3.4, 4.5]\n",
    "y_hyp = [0, 3.7, 0, 6.0, 2.9, 7.2]\n",
    "y_dyn = [6.0, 5.4, 4.6, 7.8, 7.9, 8.4]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Predicate R1 scores')\n",
    "ax = plt.subplot(111)\n",
    "# ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('R1')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_hyp, width=0.2, color='g', align='center', label='Hypothetical')\n",
    "ax.bar(ind + 0.2, y_dyn, width=0.2, color='r', align='center', label='Dynamic')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.savefig('bars-pred-r1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = expanduser('~')\n",
    "attr_map = {\"part\": \"Is.Particular\", \"kind\": \"Is.Kind\", \"abs\": \"Is.Abstract\"}\n",
    "attr_conf = {\"part\": \"Part.Confidence\", \"kind\": \"Kind.Confidence\",\n",
    "         \"abs\": \"Abs.Confidence\"}\n",
    "attrs = [\"part\", \"kind\", \"abs\"]\n",
    "attributes = ['Is.Particular.Norm', 'Is.Kind.Norm', 'Is.Abstract.Norm']\n",
    "\n",
    "datafile = home + \"/Desktop/protocols/data/noun_raw_data_norm_122218.tsv\"\n",
    "arg = pd.read_csv(datafile, sep=\"\\t\")\n",
    "\n",
    "arg['Sentence.ID'] = arg['Sentence.ID'].str.replace('sent_', '', regex=False)\n",
    "arg['Unique.ID'] = arg.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x['Span']).split(',')[0] + \"_\" + str(x['Span']).split(',')[-1], axis=1)\n",
    "arg = arg.dropna()\n",
    "arg = arg[arg['Split'].isin(['train', 'dev'])]\n",
    "arg = arg.groupby('Unique.ID', as_index=True).mean()\n",
    "\n",
    "datafile_ = home + \"/Desktop/protocols/data/spr/protoroles_eng_ud1.2_11082016.tsv\"\n",
    "spr = pd.read_csv(datafile_, sep=\"\\t\")\n",
    "# pred token is 0 indexed in SPR\n",
    "spr['Unique.ID'] = spr.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x[\"Arg.Tokens.Begin\"]) + \"_\" + str(x[\"Arg.Tokens.End\"]), axis=1)\n",
    "spr = spr[~spr['Is.Pilot']]\n",
    "spr = spr.dropna()\n",
    "\n",
    "spr = spr[spr['Split'].isin(['train', 'dev'])]\n",
    "\n",
    "# properties = ['change_of_location', 'instigation', 'partitive', 'was_for_benefit', 'existed_after', 'was_used', 'change_of_possession', 'existed_during', 'sentient', 'volition', 'change_of_state_continuous', 'awareness', 'existed_before', 'change_of_state']\n",
    "properties = ['volition', 'awareness', 'sentient', 'change_of_location', 'instigation', 'change_of_state', 'was_used', 'change_of_possession', 'partitive', 'was_for_benefit', 'existed_before', 'existed_during', 'existed_after']\n",
    "# arg_ids = list(set(arg['Unique.ID'].tolist()))\n",
    "print(\"Arg\\n\")\n",
    "for prop in properties:\n",
    "    prop_df = spr[spr['Property'] == prop]\n",
    "    prop_df.loc[:, 'Response.ridit'] = prop_df.groupby('Annotator.ID')['Response'].transform(ridit)\n",
    "    prop_df = prop_df.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "    prop_df = prop_df.loc[:, ['Unique.ID', 'Response.ridit']].dropna()\n",
    "\n",
    "    for attr in attributes:\n",
    "        prop_df.loc[:, attr] = prop_df['Unique.ID'].apply(lambda x: arg.loc[x][attr] if x in arg.index else None)\n",
    "    prop_df = prop_df.dropna()\n",
    "\n",
    "    print(prop.replace('_', ' '), '&', np.round(pearsonr(prop_df[attributes[0]].values, prop_df['Response.ridit'])[0], 2), '&', np.round(pearsonr(prop_df[attributes[1]].values, prop_df['Response.ridit'])[0], 2), '&', np.round(pearsonr(prop_df[attributes[2]].values, prop_df['Response.ridit'])[0], 2), \"\\\\\\\\\")\n",
    "\n",
    "attributes = ['Is.Particular.Norm', 'Is.Hypothetical.Norm', 'Is.Dynamic.Norm']\n",
    "datafile = home + \"/Desktop/protocols/data/pred_raw_data_norm_122218.tsv\"\n",
    "pred = pd.read_csv(datafile, sep=\"\\t\")\n",
    "pred['Sentence.ID'] = pred['Sentence.ID'].str.replace('sent_', '', regex=False)\n",
    "\n",
    "lst_col = 'Context.Span'\n",
    "x = pred.assign(**{lst_col: pred[lst_col].str.split(';')})\n",
    "pred = pd.DataFrame({col: np.repeat(x[col].values, x[lst_col].str.len()) for col in x.columns.difference([lst_col])}).assign(**{lst_col: np.concatenate(x[lst_col].values)})[x.columns.tolist()]\n",
    "pred['Unique.ID'] = pred.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x['Context.Span']).split(',')[0] + \"_\" + str(x['Context.Span']).split(',')[-1], axis=1)\n",
    "pred = pred.dropna()\n",
    "pred = pred[pred['Split'].isin(['train', 'dev'])]\n",
    "pred = pred.groupby('Unique.ID', as_index=True).mean()\n",
    "\n",
    "# pred_ids = list(set(pred['Unique.ID'].tolist()))\n",
    "print(\"\\nPred\\n\")\n",
    "for prop in properties:\n",
    "    prop_df = spr[spr['Property'] == prop]\n",
    "    prop_df.loc[:, 'Response.ridit'] = prop_df.groupby('Annotator.ID')['Response'].transform(ridit)\n",
    "    prop_df = prop_df.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "    prop_df = prop_df.loc[:, ['Unique.ID', 'Response.ridit']].dropna()\n",
    "    for attr in attributes:\n",
    "        prop_df.loc[:, attr] = prop_df['Unique.ID'].apply(lambda x: pred.loc[x][attr] if x in pred.index else None)\n",
    "    prop_df = prop_df.dropna()\n",
    "\n",
    "    print(prop.replace('_', ' '), '&', np.round(pearsonr(prop_df[attributes[0]].values, prop_df['Response.ridit'])[0], 2), '&', np.round(pearsonr(prop_df[attributes[1]].values, prop_df['Response.ridit'])[0], 2), '&', np.round(pearsonr(prop_df[attributes[2]].values, prop_df['Response.ridit'])[0], 2), \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigdig = 1\n",
    "data_arg = pd.read_csv('dev_preds_arg.tsv', sep='\\t')\n",
    "data_pred = pd.read_csv('dev_preds_pred.tsv', sep='\\t')\n",
    "\n",
    "# Do analysis on argument over here\n",
    "# print(data_arg[data_arg['kind.Pred']>1]['kind.Pred'])\n",
    "thing_words = data_arg[data_arg['Lemma'].str.contains('thing')]\n",
    "# things_words_pred = data_pred[data_pred['Lemma'].str.contains('thing')]\n",
    "# print(set(thing_words[\"Lemma\"].tolist()))\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(thing_words['Is.Particular.Norm'], hist=False, label='Part Ann').get_figure()\n",
    "sns.distplot(thing_words['part.Pred'], hist=False, label='Part Pred').get_figure()\n",
    "sns.distplot(thing_words['Is.Kind.Norm'], hist=False, label='Kind Ann').get_figure()\n",
    "sns.distplot(thing_words['kind.Pred'], hist=False, label='Kind Pred').get_figure()\n",
    "sns.distplot(thing_words['Is.Abstract.Norm'], hist=False, label='Abs Ann').get_figure()\n",
    "sns.distplot(thing_words['abs.Pred'], hist=False, label='Abs Pred').get_figure()\n",
    "plt.xlabel('Normalized score')\n",
    "plt.savefig('things.png', transparent=True)\n",
    "\n",
    "\n",
    "print([(a, len(data_arg[data_arg['POS'] == a])) for a in list(set(data_arg['POS'].tolist()))])\n",
    "# R1 and correlation based on POS and gov_rel\n",
    "print(\"\\nArg POS\")\n",
    "for pos in list(set(data_arg['POS'].tolist())):\n",
    "    data_new = data_arg[data_arg['POS'] == pos]\n",
    "    print(pos, '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['part.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['part.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['kind.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['kind.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['abs.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['abs.Pred']) * 100, sigdig), '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Kind.Norm', 'Is.Abstract.Norm']].values, data_new.loc[:, ['part.Pred', 'kind.Pred', 'abs.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "print([(a, len(data_arg[data_arg['DEPREL'] == a])) for a in list(set(data_arg['DEPREL'].tolist()))])\n",
    "print(\"\\nArg DEPREL\")\n",
    "# R1 and correlation based on POS and gov_rel\n",
    "for deprel in list(set(data_arg['DEPREL'].tolist())):\n",
    "    data_new = data_arg[data_arg['DEPREL'] == deprel]\n",
    "    print(deprel, '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['part.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['part.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['kind.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['kind.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['abs.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['abs.Pred']) * 100, sigdig), '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Kind.Norm', 'Is.Abstract.Norm']].values, data_new.loc[:, ['part.Pred', 'kind.Pred', 'abs.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "print(\"\\npred POS\")\n",
    "for pos in list(set(data_pred['POS'].tolist())):\n",
    "    data_new = data_pred[data_pred['POS'] == pos]\n",
    "    print(pos, '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['part.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['part.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Hypothetical.Norm'], data_new['hyp.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Hypothetical.Norm'], data_new['hyp.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Dynamic.Norm'], data_new['dyn.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Dynamic.Norm'], data_new['dyn.Pred']) * 100, sigdig), '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Hypothetical.Norm', 'Is.Dynamic.Norm']].values, data_new.loc[:, ['part.Pred', 'hyp.Pred', 'dyn.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "print(\"\\npred DEPREL\")\n",
    "# R1 and correlation based on POS and gov_rel\n",
    "for deprel in list(set(data_pred['DEPREL'].tolist())):\n",
    "    data_new = data_pred[data_pred['DEPREL'] == deprel]\n",
    "    print(deprel, '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['part.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['part.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Hypothetical.Norm'], data_new['hyp.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Hypothetical.Norm'], data_new['hyp.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Dynamic.Norm'], data_new['dyn.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Dynamic.Norm'], data_new['dyn.Pred']) * 100, sigdig), '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Hypothetical.Norm', 'Is.Dynamic.Norm']].values, data_new.loc[:, ['part.Pred', 'hyp.Pred', 'dyn.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "for pos in list(set(data_arg['POS'].tolist()).intersection()):\n",
    "    data_new = data_arg[data_arg['POS'] == pos]\n",
    "    data_new2 = data_pred[data_pred['POS'] == pos]\n",
    "    print(pos, '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['kind.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['kind.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['abs.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['abs.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new2['Is.Hypothetical.Norm'], data_new2['hyp.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new2['Is.Hypothetical.Norm'], data_new2['hyp.Pred']) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "for deprel in list(set(data_arg['DEPREL'].tolist())):\n",
    "    data_new = data_arg[data_arg['DEPREL'] == deprel]\n",
    "    data_new2 = data_pred[data_pred['DEPREL'] == deprel]\n",
    "    print(deprel, '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['kind.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['kind.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['abs.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['abs.Pred']) * 100, sigdig), '&', np.round(pearsonr(data_new2['Is.Hypothetical.Norm'], data_new2['hyp.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(data_new2['Is.Hypothetical.Norm'], data_new2['hyp.Pred']) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "pron_df = data_arg[data_arg['Lemma'].isin(['you', 'they'])]\n",
    "print(pron_df[pron_df['Is.Kind.Norm'] > 0]['Sentences'])\n",
    "\n",
    "hyp_df = data_pred[(data_pred['Sentences'].str.contains('if'))]\n",
    "print(data_pred[(data_pred['Sentences'].str.contains('if ', regex=False)) & (data_pred['hyp.Pred'] < -0.3)]['Sentences'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
