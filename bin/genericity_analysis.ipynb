{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from os.path import expanduser\n",
    "from sklearn import mixture\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib as mpl\n",
    "mpl.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from factslab.utility.lcsreader import LexicalConceptualStructureLexicon\n",
    "from factslab.utility import ridit, r1_score, dev_mode_group\n",
    "\n",
    "home = expanduser('~')\n",
    "%matplotlib inline\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicate train + dev data shape (36549, 29)\n",
      "Argument train + dev data shape (40883, 27)\n"
     ]
    }
   ],
   "source": [
    "pred_datafile = home + \"/Research/protocols/data/pred_raw_data_norm_122218.tsv\"\n",
    "pred_data = pd.read_csv(pred_datafile, sep=\"\\t\")\n",
    "pred_data = pred_data[pred_data['Split'].isin(['train', 'dev'])]\n",
    "print(\"Predicate train + dev data shape\", pred_data.shape)\n",
    "attributes_pred = ['Is.Particular.Norm', 'Is.Hypothetical.Norm', 'Is.Dynamic.Norm']\n",
    "\n",
    "\n",
    "arg_datafile = home + \"/Research/protocols/data/noun_raw_data_norm_122218.tsv\"\n",
    "arg_data = pd.read_csv(arg_datafile, sep=\"\\t\")\n",
    "arg_data = arg_data[arg_data['Split'].isin(['train', 'dev'])]\n",
    "attributes_arg = ['Is.Particular.Norm', 'Is.Kind.Norm', 'Is.Abstract.Norm']\n",
    "print(\"Argument train + dev data shape\", arg_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concreteness correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attributes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    935\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 936\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2d20fc10f412>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unique.ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdev_mode_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Research/protocols/data/concreteness.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributes' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2d20fc10f412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unique.ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdev_mode_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Research/protocols/data/concreteness.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist_of_lemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0m_group_selection_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 936\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m   2271\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2d20fc10f412>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unique.ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdev_mode_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhome\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Research/protocols/data/concreteness.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist_of_lemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributes' is not defined"
     ]
    }
   ],
   "source": [
    "conc_data = arg_data.groupby('Unique.ID', as_index=False).apply(lambda x: dev_mode_group(x, attributes_arg, attr_map, attr_conf, type=\"regression\")).reset_index(drop=True)\n",
    "\n",
    "path = home + \"/Research/protocols/data/concreteness.tsv\"\n",
    "conc = pd.read_csv(path, sep=\"\\t\")\n",
    "list_of_lemmas = conc['Word'].values.tolist()\n",
    "\n",
    "abs_conc = conc_data.groupby('Lemma')['Is.Abstract.Norm'].mean().to_frame().reset_index()\n",
    "abs_conc['conc'] = abs_conc['Lemma'].map(lambda x: (conc[conc['Word'] == x.lower()]['Conc.M']).values[0] if x.lower() in list_of_lemmas else -1)\n",
    "\n",
    "ini = len(abs_conc)\n",
    "abs_conc = abs_conc[abs_conc['conc'] != -1]\n",
    "print(\"Percentage of lemmas found in database:\", len(abs_conc) / ini)\n",
    "print(\"Spearman correlation: \", np.round(spearmanr(abs_conc['Is.Abstract.Norm'].values, abs_conc['conc'].values)[0], 2))\n",
    "print(\"Pearson correlation: \", np.round(pearsonr(abs_conc['Is.Abstract.Norm'].values, abs_conc['conc'].values)[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = home + \"/Research/protocols/data/verbs-English.lcs\"\n",
    "lcs = LexicalConceptualStructureLexicon(path)\n",
    "\n",
    "lcs_data = pred_data.copy()\n",
    "\n",
    "dyn_lcs = lcs_data.groupby('Lemma')['Is.Dynamic.Norm'].apply(list).to_frame().reset_index()\n",
    "dyn_lcs['lcs'] = dyn_lcs['Lemma'].map(lambda x: lcs.eventive(x.lower()) if x.lower() in lcs.verbs else -1)\n",
    "num_of_lemmas = len(dyn_lcs)\n",
    "dyn_lcs = dyn_lcs[dyn_lcs['lcs'] != -1]\n",
    "dyn_lcs.set_index('Lemma', inplace=True)\n",
    "dyn_lcs['dyn'] = dyn_lcs['Is.Dynamic.Norm'].apply(lambda x: [a > 0 for a in x])\n",
    "dyn_lcs['comp'] = dyn_lcs.apply(lambda x: 1 if set(x['dyn']).intersection(set(x['lcs'])) else 0, axis=1)\n",
    "\n",
    "print(\"Percentage of lemmas found in lcs database:\", len(dyn_lcs) / num_of_lemmas)\n",
    "print(\"They share at least one sense:\", sum(dyn_lcs['comp']) / len(dyn_lcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_ = home + \"/Research/protocols/data/it-happened_eng_ud1.2_07092017_normalized.tsv\"\n",
    "fact_data = pd.read_csv(datafile_, sep=\"\\t\")\n",
    "\n",
    "pred_data_f = pred_data.copy()\n",
    "pred_data_f['Sentence.ID'] = pred_data_f['Sentence.ID'].str.replace('sent_', '', regex=False)\n",
    "pred_data_f['Unique.ID'] = pred_data_f.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x[\"Root.Token\"]), axis=1)\n",
    "pred_data_f = pred_data_f.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "\n",
    "fact_data = fact_data[fact_data['Split'].isin(['train', 'dev'])]\n",
    "fact_data['Unique.ID'] = fact_data.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x[\"Pred.Token\"] - 1), axis=1)\n",
    "fact_data = fact_data.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "\n",
    "hyp_fact = pred_data_f.loc[:, ['Unique.ID', 'Is.Hypothetical.Norm', 'Is.Particular.Norm', 'Is.Dynamic.Norm']]\n",
    "fact_ids = fact_data['Unique.ID'].tolist()\n",
    "hyp_fact['Happened.Norm'] = hyp_fact['Unique.ID'].apply(lambda x: fact_data[fact_data['Unique.ID'] == x]['Happened.Norm'].iloc[0] if x in fact_ids else None)\n",
    "hyp_fact2 = hyp_fact.dropna()\n",
    "print(\"Overlap percentage\", np.round(len(hyp_fact2) / len(hyp_fact), 2))\n",
    "# asdf = hyp_fact2[(hyp_fact2['Is.Hypothetical.Norm']>1) & (hyp_fact2['Happened.Norm']<-1)]\n",
    "for attr in ['Is.Hypothetical.Norm', 'Is.Particular.Norm', 'Is.Dynamic.Norm']:\n",
    "    print(attr)\n",
    "    print(\"Spearman correlation: \", np.round(spearmanr(hyp_fact2[attr].values, hyp_fact2['Happened.Norm'].values)[0], 2))\n",
    "    print(\"Pearson correlation: \", np.round(pearsonr(hyp_fact2[attr].values, hyp_fact2['Happened.Norm'].values)[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [39.2, 51.6, 44.8, 58.2, 54.8, 58.0]\n",
    "y_kind = [26.0, 42.0, 33.1, 47.9, 45.6, 48.0]\n",
    "y_abs = [49.2, 34.4, 46.9, 55.8, 51.6, 56.2]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Argument correlation scores')\n",
    "ax = plt.subplot(111)\n",
    "# ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('Pearson correlation')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_kind, width=0.2, color='g', align='center', label='Kind')\n",
    "ax.bar(ind + 0.2, y_abs, width=0.2, color='r', align='center', label='Abstract')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "# plt.savefig('bars-arg-pear.png')\n",
    "\n",
    "\n",
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [16.3, 20.9, 20.3, 27.5, 24.7, 28.5]\n",
    "y_hyp = [13.8, 38.3, 22.9, 42.2, 38.8, 42.0]\n",
    "y_dyn = [33.2, 31.5, 29.4, 38.3, 37.5, 38.8]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Predicate correlation scores')\n",
    "ax = plt.subplot(111)\n",
    "# ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('Pearson correlation')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_hyp, width=0.2, color='g', align='center', label='Hypothetical')\n",
    "ax.bar(ind + 0.2, y_dyn, width=0.2, color='r', align='center', label='Dynamic')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "# plt.savefig('bars-pred-pear.png')\n",
    "\n",
    "\n",
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [7.9, 13.1, 9.7, 15.7, 13.5, 15.8]\n",
    "y_kind = [2.1, 8.9, 2.6, 11.6, 10.8, 11.4]\n",
    "y_abs = [14.1, 6.3, 11.9, 17.3, 15.2, 17.7]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Argument R1 scores')\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('R1')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_kind, width=0.2, color='g', align='center', label='Kind')\n",
    "ax.bar(ind + 0.2, y_abs, width=0.2, color='r', align='center', label='Abstract')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "# plt.savefig('bars-arg-r1.png')\n",
    "\n",
    "\n",
    "x = ['Token', 'Type', 'GloVe', 'ELMO', 'Hand', 'All']\n",
    "ind = np.arange(len(x))\n",
    "y_part = [1.2, 2.7, 2.2, 3.9, 3.4, 4.5]\n",
    "y_hyp = [0, 3.7, 0, 6.0, 2.9, 7.2]\n",
    "y_dyn = [6.0, 5.4, 4.6, 7.8, 7.9, 8.4]\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Predicate R1 scores')\n",
    "ax = plt.subplot(111)\n",
    "# ax.set_xlabel('Model features')\n",
    "ax.set_ylabel('R1')\n",
    "ax.bar(ind - 0.2, y_part, width=0.2, color='b', align='center', label='Particular')\n",
    "ax.bar(x, y_hyp, width=0.2, color='g', align='center', label='Hypothetical')\n",
    "ax.bar(ind + 0.2, y_dyn, width=0.2, color='r', align='center', label='Dynamic')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "# plt.savefig('bars-pred-r1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_data_spr = arg_data.copy()\n",
    "arg_data_spr['Sentence.ID'] = arg_data_spr['Sentence.ID'].str.replace('sent_', '', regex=False)\n",
    "arg_data_spr['Unique.ID'] = arg_data_spr.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x['Span']).split(',')[0] + \"_\" + str(x['Span']).split(',')[-1], axis=1)\n",
    "arg_data_spr = arg_data_spr.dropna()\n",
    "arg_data_spr = arg_data_spr.groupby('Unique.ID', as_index=True).mean()\n",
    "\n",
    "datafile_ = home + \"/Research/protocols/data/spr/protoroles_eng_ud1.2_11082016.tsv\"\n",
    "spr = pd.read_csv(datafile_, sep=\"\\t\")\n",
    "# pred_data token is 0 indexed in SPR\n",
    "spr['Unique.ID'] = spr.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x[\"Arg.Tokens.Begin\"]) + \"_\" + str(x[\"Arg.Tokens.End\"]), axis=1)\n",
    "spr = spr[~spr['Is.Pilot']]\n",
    "spr = spr.dropna()\n",
    "\n",
    "spr = spr[spr['Split'].isin(['train', 'dev'])]\n",
    "\n",
    "properties = ['awareness', 'volition', 'sentient', 'instigation', 'existed_before', 'existed_during', 'existed_after', 'was_for_benefit', 'change_of_location', 'change_of_state', 'was_used', 'change_of_possession', 'partitive']\n",
    "\n",
    "print(\"Arg\\n\")\n",
    "for prop in properties:\n",
    "    prop_df = spr[spr['Property'] == prop]\n",
    "    prop_df.loc[:, 'Response.ridit'] = prop_df.groupby('Annotator.ID')['Response'].transform(ridit)\n",
    "    prop_df = prop_df.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "    prop_df = prop_df.loc[:, ['Unique.ID', 'Response.ridit']].dropna()\n",
    "\n",
    "    for attr in attributes_arg:\n",
    "        prop_df.loc[:, attr] = prop_df['Unique.ID'].apply(lambda x: arg_data_spr.loc[x][attr] if x in arg_data_spr.index else None)\n",
    "\n",
    "    prop_df = prop_df.dropna()\n",
    "    print(prop.replace('_', ' '), \n",
    "          '&', np.round(spearmanr(prop_df[attributes_arg[0]].values, prop_df['Response.ridit'].values)[0], 2), \n",
    "          '&', np.round(spearmanr(prop_df[attributes_arg[1]].values, prop_df['Response.ridit'].values)[0], 2), \n",
    "          '&', np.round(spearmanr(prop_df[attributes_arg[2]].values, prop_df['Response.ridit'].values)[0], 2), \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "pred_data_spr = pred_data.copy()\n",
    "pred_data_spr['Sentence.ID'] = pred_data['Sentence.ID'].str.replace('sent_', '', regex=False)\n",
    "\n",
    "lst_col = 'Context.Span'\n",
    "x = pred_data_spr.assign(**{lst_col: pred_data[lst_col].str.split(';')})\n",
    "pred_data_spr = pd.DataFrame({col: np.repeat(x[col].values, x[lst_col].str.len()) for col in x.columns.difference([lst_col])}).assign(**{lst_col: np.concatenate(x[lst_col].values)})[x.columns.tolist()]\n",
    "pred_data_spr['Unique.ID'] = pred_data_spr.apply(lambda x: str(x['Sentence.ID']) + \"_\" + str(x['Context.Span']).split(',')[0] + \"_\" + str(x['Context.Span']).split(',')[-1], axis=1)\n",
    "pred_data_spr = pred_data_spr.dropna()\n",
    "pred_data_spr = pred_data_spr.groupby('Unique.ID', as_index=True).mean()\n",
    "\n",
    "print(\"\\nPred\\n\")\n",
    "for prop in properties:\n",
    "    prop_df = spr[spr['Property'] == prop]\n",
    "    prop_df.loc[:, 'Response.ridit'] = prop_df.groupby('Annotator.ID')['Response'].transform(ridit)\n",
    "    prop_df = prop_df.groupby('Unique.ID', as_index=False).mean().reset_index(drop=True)\n",
    "    prop_df = prop_df.loc[:, ['Unique.ID', 'Response.ridit']].dropna()\n",
    "    for attr in attributes_pred:\n",
    "        prop_df.loc[:, attr] = prop_df['Unique.ID'].apply(lambda x: pred_data_spr.loc[x][attr] if x in pred_data_spr.index else None)\n",
    "    prop_df = prop_df.dropna()\n",
    "\n",
    "    print(prop.replace('_', ' '), \n",
    "          '&', np.round(spearmanr(prop_df[attributes_pred[0]].values, prop_df['Response.ridit'])[0], 2), \n",
    "          '&', np.round(spearmanr(prop_df[attributes_pred[1]].values, prop_df['Response.ridit'])[0], 2), \n",
    "          '&', np.round(spearmanr(prop_df[attributes_pred[2]].values, prop_df['Response.ridit'])[0], 2), \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigdig = 1\n",
    "arg_dev = pd.read_csv('dev_preds_arg.tsv', sep='\\t')\n",
    "pred_dev = pd.read_csv('dev_preds_pred.tsv', sep='\\t')\n",
    "\n",
    "attributes_arg_dev = ['Is.Particular.Pred', 'Is.Kind.Pred', 'Is.Abstract.Pred']\n",
    "attributes_pred_dev = ['Is.Particular.Pred', 'Is.Hypothetical.Pred', 'Is.Dynamic.Pred']\n",
    "\n",
    "pron_df = arg_dev[arg_dev['Lemma'].isin(['you', 'they'])]\n",
    "print(\"Pronomial sentences containing you/they with high Is.Kind values\\n\")\n",
    "print(pron_df[pron_df['Is.Kind.Norm'] > 0]['Sentences'].sample(5), \"\\n\")\n",
    "\n",
    "hyp_df = pred_dev[(pred_dev['Sentences'].str.contains('if'))]\n",
    "print(\"Conditional sentences(with if) with low Is.Hypothetical scores\\n\")\n",
    "print(pred_dev[(pred_dev['Sentences'].str.contains('if ', regex=False)) & \n",
    "               (pred_dev['Is.Hypothetical.Pred'] < -0.3)]['Sentences'].sample(5), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update({'font.size': 15})\n",
    "thing_words = arg_dev[arg_dev['Lemma'].str.contains('thing')]\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(thing_words['Is.Particular.Norm'], hist=False, label='Part Ann').get_figure()\n",
    "sns.distplot(thing_words['Is.Particular.Pred'], hist=False, label='Part Pred').get_figure()\n",
    "sns.distplot(thing_words['Is.Kind.Norm'], hist=False, label='Kind Ann').get_figure()\n",
    "sns.distplot(thing_words['Is.Kind.Pred'], hist=False, label='Kind Pred').get_figure()\n",
    "sns.distplot(thing_words['Is.Abstract.Norm'], hist=False, label='Abs Ann').get_figure()\n",
    "sns.distplot(thing_words['Is.Abstract.Pred'], hist=False, label='Abs Pred').get_figure()\n",
    "plt.xlabel('Normalized score')\n",
    "plt.show()\n",
    "# plt.savefig('things.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS and DEPREL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nArg POS\")\n",
    "pprint([(a, len(arg_dev[arg_dev['POS'] == a])) for a in list(set(arg_dev['POS'].tolist()))])\n",
    "for pos in list(set(arg_dev['POS'].tolist())):\n",
    "    data_new = arg_dev[arg_dev['POS'] == pos]\n",
    "    print(pos, '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred']) * 100, sigdig), \n",
    "          '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred']) * 100, sigdig),\n",
    "          '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred']) * 100, sigdig),\n",
    "          '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Kind.Norm', 'Is.Abstract.Norm']].values, data_new.loc[:, ['Is.Particular.Pred', 'Is.Kind.Pred', 'Is.Abstract.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "print(\"\\nArg DEPREL\")\n",
    "pprint([(a, len(arg_dev[arg_dev['DEPREL'] == a])) for a in list(set(arg_dev['DEPREL'].tolist()))])\n",
    "for deprel in list(set(arg_dev['DEPREL'].tolist())):\n",
    "    data_new = arg_dev[arg_dev['DEPREL'] == deprel]\n",
    "    print(deprel, '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred'])[0] * 100, sigdig),\n",
    "          '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred']) * 100, sigdig), \n",
    "          '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred']) * 100, sigdig), \n",
    "          '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred']) * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Kind.Norm', 'Is.Abstract.Norm']].values, data_new.loc[:, ['Is.Particular.Pred', 'Is.Kind.Pred', 'Is.Abstract.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "print(\"\\nPred POS\")\n",
    "pprint([(a, len(pred_dev[pred_dev['POS'] == a])) for a in list(set(pred_dev['POS'].tolist()))])\n",
    "for pos in list(set(pred_dev['POS'].tolist())):\n",
    "    data_new = pred_dev[pred_dev['POS'] == pos]\n",
    "    print(pos, '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred']) * 100, sigdig),\n",
    "          '&', np.round(pearsonr(data_new['Is.Hypothetical.Norm'], data_new['Is.Hypothetical.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Hypothetical.Norm'], data_new['Is.Hypothetical.Pred']) * 100, sigdig), \n",
    "          '&', np.round(pearsonr(data_new['Is.Dynamic.Norm'], data_new['Is.Dynamic.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Dynamic.Norm'], data_new['Is.Dynamic.Pred']) * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Hypothetical.Norm', 'Is.Dynamic.Norm']].values, data_new.loc[:, ['Is.Particular.Pred', 'Is.Hypothetical.Pred', 'Is.Dynamic.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "print(\"\\nPred DEPREL\")\n",
    "pprint([(a, len(pred_dev[pred_dev['DEPREL'] == a])) for a in list(set(pred_dev['DEPREL'].tolist()))])\n",
    "for deprel in list(set(pred_dev['DEPREL'].tolist())):\n",
    "    data_new = pred_dev[pred_dev['DEPREL'] == deprel]\n",
    "    print(deprel,\n",
    "          '&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred']) * 100, sigdig), \n",
    "          '&', np.round(pearsonr(data_new['Is.Hypothetical.Norm'], data_new['Is.Hypothetical.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Hypothetical.Norm'], data_new['Is.Hypothetical.Pred']) * 100, sigdig),\n",
    "          '&', np.round(pearsonr(data_new['Is.Dynamic.Norm'], data_new['Is.Dynamic.Pred'])[0] * 100, sigdig),\n",
    "          '&', np.round(r1_score(data_new['Is.Dynamic.Norm'], data_new['Is.Dynamic.Pred']) * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Hypothetical.Norm', 'Is.Dynamic.Norm']].values, data_new.loc[:, ['Is.Particular.Pred', 'Is.Hypothetical.Pred', 'Is.Dynamic.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "\n",
    "# for pos in list(set(arg_dev['POS'].tolist()).intersection(set(pred_dev['POS'].tolist()))):\n",
    "#     data_new = arg_dev[arg_dev['POS'] == pos]\n",
    "#     data_new2 = pred_dev[pred_dev['POS'] == pos]\n",
    "#     print(pos, \n",
    "#           '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred'])[0] * 100, sigdig), \n",
    "#           '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred']) * 100, sigdig), \n",
    "#           '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred'])[0] * 100, sigdig), \n",
    "#           '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred']) * 100, sigdig), \n",
    "#           '&', np.round(pearsonr(data_new2['Is.Hypothetical.Norm'], data_new2['Is.Hypothetical.Pred'])[0] * 100, sigdig), \n",
    "#           '&', np.round(r1_score(data_new2['Is.Hypothetical.Norm'], data_new2['Is.Hypothetical.Pred']) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n",
    "# for deprel in list(set(arg_dev['POS'].tolist()).intersection(set(pred_dev['POS'].tolist()))):\n",
    "#     data_new = arg_dev[arg_dev['DEPREL'] == deprel]\n",
    "#     data_new2 = pred_dev[pred_dev['DEPREL'] == deprel]\n",
    "#     print(deprel, \n",
    "#           '&', np.round(pearsonr(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred'])[0] * 100, sigdig), \n",
    "#           '&', np.round(r1_score(data_new['Is.Kind.Norm'], data_new['Is.Kind.Pred']) * 100, sigdig), \n",
    "#           '&', np.round(pearsonr(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred'])[0] * 100, sigdig), \n",
    "#           '&', np.round(r1_score(data_new['Is.Abstract.Norm'], data_new['Is.Abstract.Pred']) * 100, sigdig), \n",
    "#           '&', np.round(pearsonr(data_new2['Is.Hypothetical.Norm'], data_new2['Is.Hypothetical.Pred'])[0] * 100, sigdig), \n",
    "#           '&', np.round(r1_score(data_new2['Is.Hypothetical.Norm'], data_new2['Is.Hypothetical.Pred']) * 100, sigdig), \"\\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev[(pred_dev['Is.Particular.Norm']<-0.2) & (pred_dev['Is.Hypothetical.Norm']<-0.2)][['Unique.ID', 'Sentences']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(np.round(pearsonr(pred_dev['Is.Particular.Norm'], pred_dev['Is.Hypothetical.Norm'])[0] * 100, sigdig))\n",
    "print(np.round(pearsonr(pred_dev['Is.Particular.Pred'], pred_dev['Is.Hypothetical.Pred'])[0] * 100, sigdig))\n",
    "print(np.round(pearsonr(pred_dev['Is.Particular.Norm'], pred_dev['Is.Hypothetical.Pred'])[0] * 100, sigdig))\n",
    "print(np.round(pearsonr(pred_dev['Is.Particular.Pred'], pred_dev['Is.Hypothetical.Norm'])[0] * 100, sigdig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigdig=3\n",
    "def create_corr_df(df, attributes):\n",
    "    '''\n",
    "        Creates a dataframe of correlations among attributes in df\n",
    "    '''\n",
    "    df_corr = {}\n",
    "    for attr in attributes:\n",
    "        df_corr[attr] = {}\n",
    "        for attr1 in attributes:\n",
    "            df_corr[attr][attr1] = np.round(pearsonr(df[attr], df[attr1])[0], sigdig)\n",
    "    return pd.DataFrame(df_corr)\n",
    "\n",
    "def create_dist(df, attributes):\n",
    "    '''\n",
    "        Finds mean, median, variance of each attribute in df\n",
    "    '''\n",
    "    df_props = {}\n",
    "    for attr in attributes:\n",
    "        df_props[attr] = {}\n",
    "        df_props[attr]['mean'] = np.round(np.mean(df[attr]), sigdig)\n",
    "        df_props[attr]['median'] = np.round(np.median(df[attr]), sigdig) \n",
    "        df_props[attr]['var'] = np.round(np.var(df[attr]), sigdig)\n",
    "    return df_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arg_data.shape)\n",
    "display(pd.DataFrame(create_corr_df(arg_data, attributes_arg)))\n",
    "display(pd.DataFrame(create_dist(arg_data, attributes_arg)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[20, 5], nrows=1, ncols=3, squeeze=False, sharey='row')\n",
    "for i in range(3):\n",
    "    sns.distplot(arg_data[attributes_arg[i]], ax=ax[0][i]).get_figure()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(create_corr_df(arg_dev, attributes_arg_dev)))\n",
    "display(pd.DataFrame(create_dist(arg_dev, attributes_arg_dev)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[20, 5], nrows=1, ncols=3, squeeze=False, sharey='row')\n",
    "for i in range(3):\n",
    "    sns.distplot(arg_dev[attributes_arg_dev[i]], ax=ax[0][i]).get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(create_corr_df(pred_data, attributes_pred)))\n",
    "display(pd.DataFrame(create_dist(pred_data, attributes_pred)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[20, 5], nrows=1, ncols=3, squeeze=False, sharey='row')\n",
    "for i in range(3):\n",
    "    sns.distplot(pred_data[attributes_pred[i]], ax=ax[0][i]).get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(create_corr_df(pred_dev, attributes_pred_dev)))\n",
    "display(pd.DataFrame(create_dist(pred_dev, attributes_pred_dev)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[20, 5], nrows=1, ncols=3, squeeze=False, sharey='row')\n",
    "for i in range(3):\n",
    "    sns.distplot(pred_dev[attributes_pred_dev[i]], ax=ax[0][i]).get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "display(arg_dev[(arg_dev['Is.Particular.Norm']<-0.2) & (arg_dev['Is.Abstract.Norm']<-0.2)][['Unique.ID', 'Sentences', 'Word', 'Is.Particular.Norm', 'Is.Particular.Pred', 'Is.Abstract.Norm', 'Is.Abstract.Pred', 'Is.Kind.Norm', 'Is.Kind.Pred']].sort_values(by=['Is.Particular.Pred', 'Is.Abstract.Pred'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = arg_dev[(arg_dev['POS']=='PROPN')]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[20, 5], nrows=1, ncols=3, squeeze=False, sharey='row')\n",
    "for i in range(3):\n",
    "    sns.distplot(upper[attributes_arg[i]], ax=ax[0][i]).get_figure()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "display(pred_dev[(pred_dev['Is.Particular.Pred']>0.1) & (pred_dev['Is.Dynamic.Pred']>0.1)][['Unique.ID', 'Sentences', 'Word', 'Is.Particular.Norm', 'Is.Particular.Pred', 'Is.Hypothetical.Norm', 'Is.Hypothetical.Pred', 'Is.Dynamic.Norm', 'Is.Dynamic.Pred']].sort_values(by=['Is.Particular.Pred', 'Is.Dynamic.Pred'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clausal versus other DEPREL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_deprel = ['csubj', 'ccomp', 'xcomp', 'advcl', 'acl']\n",
    "other_deprel = ['root', 'conj', 'parataxis']\n",
    "\n",
    "pprint([(a, len(pred_dev[pred_dev['DEPREL'].isin(a)])) for a in [clause_deprel, other_deprel]])\n",
    "\n",
    "for deprel_set in [clause_deprel, other_deprel]:\n",
    "    data_new = pred_dev[pred_dev['DEPREL'].isin(deprel_set)]\n",
    "    print('&', np.round(pearsonr(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Particular.Norm'], data_new['Is.Particular.Pred']) * 100, sigdig), \n",
    "          '&', np.round(pearsonr(data_new['Is.Hypothetical.Norm'], data_new['Is.Hypothetical.Pred'])[0] * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new['Is.Hypothetical.Norm'], data_new['Is.Hypothetical.Pred']) * 100, sigdig),\n",
    "          '&', np.round(pearsonr(data_new['Is.Dynamic.Norm'], data_new['Is.Dynamic.Pred'])[0] * 100, sigdig),\n",
    "          '&', np.round(r1_score(data_new['Is.Dynamic.Norm'], data_new['Is.Dynamic.Pred']) * 100, sigdig), \n",
    "          '&', np.round(r1_score(data_new.loc[:, ['Is.Particular.Norm', 'Is.Hypothetical.Norm', 'Is.Dynamic.Norm']].values, data_new.loc[:, ['Is.Particular.Pred', 'Is.Hypothetical.Pred', 'Is.Dynamic.Pred']].values) * 100, sigdig), \"\\\\\\\\\")\n",
    "print(np.round(pearsonr(pred_dev['Is.Particular.Norm'], pred_dev['Is.Particular.Pred'])[0] * 100, sigdig), '&', np.round(r1_score(pred_dev['Is.Particular.Norm'], pred_dev['Is.Particular.Pred']) * 100, sigdig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
